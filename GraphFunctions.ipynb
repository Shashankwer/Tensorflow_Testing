{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphFunctions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2LuNUMC17qKKDyEs6VeFR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashankwer/Tensorflow_Testing/blob/master/GraphFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBFMJN6XkHvb",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to graphs and tf.function\n",
        "\n",
        "\n",
        "### Graphs? \n",
        "\n",
        "Eager execution allows tensorflow to run operation by operation and returns the results back to Python. By Eager execution Tensorflow takes advantage of GPUs allowing to place variables, tensors and even operations on GPUs and TPUs. It is also easier to debug. \n",
        "\n",
        "Executing op by op in Python prevents the host acceleration which is otherwise available. If one can extract computation from Python one can make them into a graph. \n",
        "\n",
        "Graphs are the structures that contain a set of tf.Operation objects, which represents units of computation; and tf.Tensor object, which represent the units of data that flows between the operation. \n",
        "\n",
        "Graph are defined in a tf.Graph context. \n",
        "\n",
        "### The benefits of graphs: \n",
        "\n",
        "With graph one can have a great deal of flexibility. One can use Tensorflow graph in environments that do not have a Python interpreter, like mobile applications, embedded devices and backend services. Tensorflow uses graphs as the format for saved models when it exports them from python. \n",
        "\n",
        "Graphs are also easily optimized allowing the compiler to do the transformations like \n",
        "\n",
        "1. Statistically infer the value of the tensor by folding constatnnode in the computation\n",
        "2. Seperate subparts of a computation that are independent and split them between threads or devices. \n",
        "3. Simplify arithematic operations by eliminating common subexpressions. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROH3T5bokFJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "from datetime import datetime\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X8qyuFfmraR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def function_to_get_faster(x,y,b):\n",
        "  x = tf.matmul(x,y)\n",
        "  x = x + b\n",
        "  return x"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65lWMkEOnANI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_function_that_uses_graph = tf.function(function_to_get_faster)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaJBcTsYnGFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca4c5714-3dd5-4eb2-8490-4eb65cbea467"
      },
      "source": [
        "x1 = tf.constant([[1.0,2.0]])\n",
        "y1 = tf.constant([[2.0],[3.0]])\n",
        "b1 = tf.constant(4.0)\n",
        "a_function_that_uses_graph(x1,y1,b1).numpy()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[12.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avuFliYBnYrO",
        "colab_type": "text"
      },
      "source": [
        "tf.function-ized functions are Python callbales and are equivalent to the python classes. They have a particular class \n",
        "`python.eager.def_function.Function` but to us its simply a non traced function version. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlrGeKsWnYLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inner_function(x,y,b):\n",
        "  #print(x,y,b)\n",
        "  x = tf.matmul(x,y)\n",
        "  x = x+b\n",
        "  #print(x)\n",
        "  return x\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecwJk9cdnUvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b956b66a-c01d-41a7-dd86-87b83fa85489"
      },
      "source": [
        "@tf.function\n",
        "def outer_function(x):\n",
        "  #print(x)\n",
        "  y = tf.constant([[2.0],[3.0]])\n",
        "  b = tf.constant(3.0)\n",
        "  return inner_function(x,y,b)\n",
        "\n",
        "#tf.function creates a graph.\n",
        "outer_function(tf.constant([[1.0, 2.0]])).numpy()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev2VrlbbpBkb",
        "colab_type": "text"
      },
      "source": [
        "Flow Controls and side effects: \n",
        "\n",
        "Flow controls and loops are converted into Tensorflow via `tf.autograph` which makes use of methods standardizing loop constructs, unrolling and AST manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tEGxvKdoKsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a334f136-22cb-4d0a-85e3-1e84d6d4fb11"
      },
      "source": [
        "def my_function(x):\n",
        "  if tf.reduce_sum(x)<=1:\n",
        "    return x*x\n",
        "  else:\n",
        "    return x-1\n",
        "a_function = tf.function(my_function)\n",
        "print(\"First Branch with graph\",a_function(tf.constant(1.0)).numpy())\n",
        "print(\"Second Branch with graph\",a_function(tf.constant([5.0,6.0])).numpy())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Branch with graph 1.0\n",
            "Second Branch with graph [4. 5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6OaWp45pkZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "05d1d422-f8f8-41d3-a64b-38a86093a156"
      },
      "source": [
        "print(tf.autograph.to_code(my_function))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def tf__my_function(x):\n",
            "    with ag__.FunctionScope('my_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
            "        do_return = False\n",
            "        retval_ = ag__.UndefinedReturnValue()\n",
            "\n",
            "        def get_state():\n",
            "            return (retval_, do_return)\n",
            "\n",
            "        def set_state(vars_):\n",
            "            nonlocal retval_, do_return\n",
            "            (retval_, do_return) = vars_\n",
            "\n",
            "        def if_body():\n",
            "            nonlocal retval_, do_return\n",
            "            try:\n",
            "                do_return = True\n",
            "                retval_ = (ag__.ld(x) * ag__.ld(x))\n",
            "            except:\n",
            "                do_return = False\n",
            "                raise\n",
            "\n",
            "        def else_body():\n",
            "            nonlocal retval_, do_return\n",
            "            try:\n",
            "                do_return = True\n",
            "                retval_ = (ag__.ld(x) - 1)\n",
            "            except:\n",
            "                do_return = False\n",
            "                raise\n",
            "        ag__.if_stmt((ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) <= 1), if_body, else_body, get_state, set_state, ('retval_', 'do_return'), 2)\n",
            "        return fscope.ret(retval_, do_return)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taYjWFVtqITB",
        "colab_type": "text"
      },
      "source": [
        "### Seeing the speedup\n",
        "\n",
        "Just wrapping a tensor-saped function `tf.function` does not automatically speeds up your code. For small functions called a few times on a single machine the overhead of calling a grpah or fragmented graph may dominate runtime. Also if most of the computation already happened on the accelerator, such as stacks of GPU heavy computation, the graph speedup wont be large.\n",
        "\n",
        "For complicated computation, graph can provide a significant speedup. This is because graph can reduce the Python device to device communication and performs some speedups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PuzzAEcpvBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequentialModel(tf.keras.Model):\n",
        "  def __init__(self,**kwargs):\n",
        "    super(SequentialModel,self).__init__(**kwargs)\n",
        "    self.flatten = tf.keras.layers.Flatten(input_shape=(28,28))\n",
        "    self.dense_1 = tf.keras.layers.Dense(128,activation='relu')\n",
        "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "    self.dense_2 = tf.keras.layers.Dense(10)\n",
        "  \n",
        "  def call(self,x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense_1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense_2(x)\n",
        "    return x\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slW_25frrV1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "526044f4-3d18-4a13-ec06-c04865626d2d"
      },
      "source": [
        "input_data = tf.random.uniform([60,28,28])\n",
        "eager_model = SequentialModel()\n",
        "graph_model = tf.function(eager_model)\n",
        "print(\"Eager Time:\", timeit.timeit(lambda: eager_model(input_data),number=10000))\n",
        "print(\"Graph Time:\", timeit.timeit(lambda: graph_model(input_data),number=10000))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eager Time: 9.851775883999835\n",
            "Graph Time: 5.827727493000111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2vski2viHr",
        "colab_type": "text"
      },
      "source": [
        "## Polymorphic functions\n",
        "\n",
        "A function stores the different kinds of datatyoe by which the grpah can be invoked. The function then stores the tf.Graph corresponding to the trace concrete_cuntion. If the function has already been traced with that kind of argument, one can just get the pretraced graph\n",
        "\n",
        "Conceptually:\n",
        "  1. A tf.Graph is the raw datastructure describing a computation\n",
        "  2. A Function is a caching, tracing, dispatching over concrete function\n",
        "  3. A ConcreteFunction is an eager compatible wrapped around a graph that allows to execute graph from Python\n",
        "\n",
        "By default the tensorflow function execute with tf.Graph or with.Graph().as_default(). This measn that one is likely running in a graph context. Core functions in Tensorflow use graph contexts, such as Keras's model_fit()\n",
        "\n",
        "For converting the function back to eager mode one can make use of:\n",
        "1. Call models and layers directly as callables\n",
        "2. When using Keras compile/fit at compile time use model.compile(run_eagerly=True)\n",
        "3. Setting the global execution mode via tf.config.run_functions_eagerly(True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFub6_96rx0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EagerLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super(EagerLayer,self).__init__(**kwargs)\n",
        "    #some initialization\n",
        "  \n",
        "  def call(self,inputs):\n",
        "    print(\"\\nCurrently running eagerly\",str(datetime.now))\n",
        "    return inputs"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uwl7uc5xsXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequentialModel(tf.keras.Model):\n",
        "  def __init__(self,**kwargs):\n",
        "    super(SequentialModel,self).__init__(**kwargs)\n",
        "    self.flatten = tf.keras.layers.Flatten(input_shape=(28,28))\n",
        "    self.dense_1 = tf.keras.layers.Dense(128,activation='relu')\n",
        "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "    self.dense_2 = tf.keras.layers.Dense(10)\n",
        "    self.eager = EagerLayer()\n",
        "  \n",
        "  def call(self,x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense_1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense_2(x)\n",
        "    return self.eager(x)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5aMrXR0ybbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SequentialModel()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwTY_pahyfaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = tf.random.uniform([60,28,28])\n",
        "labels = tf.random.uniform([60])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlsLlLQqywNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(run_eagerly=False,loss=loss_fn)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ8IrT9Zy3j_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "438d1d3c-6526-43af-a44f-077921ca40b1"
      },
      "source": [
        "model.fit(input_data,labels,epochs=3)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.9271\n",
            "Epoch 2/3\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 3/3\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 4.8072e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07a4735be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h09PmTSay7tb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a2dd7302-a549-4d9f-abdb-d792f3b64e1f"
      },
      "source": [
        "print(\"Running eagerly\")\n",
        "model.compile(run_eagerly=True,loss=loss_fn)\n",
        "model.fit(input_data,labels,epochs=3)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running eagerly\n",
            "Epoch 1/3\n",
            "\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1.5646e-07\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.3709e-07\n",
            "Epoch 2/3\n",
            "\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 2.6077e-08\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.3842e-08\n",
            "Epoch 3/3\n",
            "\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1.1176e-08\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.3578e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07a47eeba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2LnvInozQSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "dfcb9fb3-cfff-4369-c412-f556cd41af6c"
      },
      "source": [
        "\"\"\"\n",
        "Using run_functions_eagerly:\n",
        "\n",
        "You can also globally set everything to run eagerly\n",
        "\"\"\"\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(\"Running all functions eagerly\")\n",
        "\n",
        "polymorphic_functions = tf.function(model)\n",
        "\n",
        "print(polymorphic_functions.get_concrete_function(input_data))\n",
        "\n",
        "result = polymorphic_function(input_data)\n",
        "result = polymorphic_function(input_data)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running all functions eagerly\n",
            "\n",
            "Currently running eagerly <built-in method now of type object at 0xa33e60>\n",
            "ConcreteFunction function(self)\n",
            "  Args:\n",
            "    self: float32 Tensor, shape=(60, 28, 28)\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(60, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-708d3530378b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolymorphic_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolymorphic_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolymorphic_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'polymorphic_function' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Smlle_e0I5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ddc485ab-8ef9-4492-cee8-d13939e77961"
      },
      "source": [
        "tf.config.experimental_run_functions_eagerly(False)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-48-6f08e7c936eb>:1: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0MoymBI0icE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}